# G1-OpenClaw Bridge

Talk to your AI through smart glasses. This bridge connects [Even Realities G1](https://www.evenrealities.com/) glasses to [OpenClaw](https://openclaw.ai), so you can speak a question and see the answer appear on your lens display ‚Äî hands-free, in real time.

## What it does

You wear the glasses. You say something. Your AI assistant hears it, thinks about it, and shows a short answer right in your field of vision. It also forwards your phone notifications to the glasses and lets you control things with head gestures.

**Features:**

- üéôÔ∏è **Voice ‚Üí AI ‚Üí Display** ‚Äî Speak naturally, get answers on your HUD
- üîÑ **Copilot Mode** ‚Äî AI listens silently to your conversations and only chimes in with useful context (facts, corrections, suggestions)
- üß† **LLM Pre-Filter** ‚Äî Cheap model (Claude Haiku) filters copilot transcripts before they reach your main AI ‚Äî saves ~85-95% of API costs
- üìù **Transcript Logging** ‚Äî All conversations logged for later summarization and recall
- üì± **Phone Notifications** ‚Äî See incoming messages, calls, and app alerts on the glasses
- üñºÔ∏è **Bitmap Push** ‚Äî Send custom images (QR codes, charts, pixel art) to the display
- üéØ **Head-Up Toggle** ‚Äî Look up for 5 seconds to toggle the microphone on/off
- üìä **Dashboard & Debug API** ‚Äî Status card on glasses + detailed pipeline diagnostics via HTTP
- üîÅ **Auto-reconnect** ‚Äî Recovers from network drops and stream errors automatically
- üõ°Ô∏è **Notification Dedup & Blocklist** ‚Äî No spam on your face

## How it works

```
You speak ‚Üí G1 Mic ‚Üí MentraOS Cloud (speech-to-text)
  ‚Üí This Bridge ‚Üí OpenClaw Gateway (your AI agent)
  ‚Üí Reply via WebSocket ‚Üí Bridge ‚Üí G1 HUD Display
```

In **Copilot Mode**, there's an extra filtering step:

```
Transcript ‚Üí Bridge ‚Üí Log file (always)
                    ‚Üí LLM Pre-Filter (Haiku, ~200ms, stateless)
                         ‚îú‚îÄ‚îÄ SKIP ‚Üí done (saved an expensive API call!)
                         ‚îî‚îÄ‚îÄ RELEVANT ‚Üí OpenClaw (Opus/Sonnet) with conversation context
```

The bridge runs on a server (a VPS, a Raspberry Pi, your laptop ‚Äî anything with Node/Bun). It talks to MentraOS (Even Realities' cloud platform) for speech-to-text and display control, and to OpenClaw's Gateway WebSocket for AI chat.

## Requirements

- [Even Realities G1](https://www.evenrealities.com/) smart glasses
- [MentraOS Developer Account](https://developer.mentra.glass/) with an app + API key
- [OpenClaw](https://openclaw.ai) instance running with Gateway WebSocket enabled
- [Bun](https://bun.sh/) runtime (v1.0+)
- A server reachable from the internet (MentraOS sends webhooks to your bridge)
- *(Optional)* An API endpoint for the copilot LLM filter (e.g. Claude Haiku via Azure, Anthropic, or any OpenAI-compatible API)

## Setup

### 1. Clone and install

```bash
git clone https://github.com/johannboehme/HexMentraBridge.git
cd HexMentraBridge
bun install
```

### 2. Configure

```bash
cp .env.example .env
```

Edit `.env` with your credentials:

| Variable | What it is |
|----------|------------|
| `PACKAGE_NAME` | Your MentraOS app package name (from Developer Console) |
| `MENTRAOS_API_KEY` | Your MentraOS API key (from Developer Console) |
| `OPENCLAW_WS_URL` | WebSocket URL of your OpenClaw Gateway (default: `ws://localhost:18789`) |
| `OPENCLAW_GW_TOKEN` | Your OpenClaw Gateway token |
| `PORT` | Port for MentraOS webhooks (default: 3000) |
| `PUSH_PORT` | Port for the Push API (default: 3001) |
| `PUSH_BIND` | Bind address for Push API ‚Äî `127.0.0.1` for local only, `0.0.0.0` for external (default: `127.0.0.1`) |
| `PUSH_TOKEN` | Auth token for Push API when exposed externally (generate with `openssl rand -hex 24`) |
| `NOTIF_BLOCKLIST` | Comma-separated app names to suppress (e.g. `System UI,Google Play Store`) |
| `FILTER_LLM_URL` | *(Optional)* API endpoint for copilot pre-filter LLM (e.g. Azure Anthropic Messages endpoint) |
| `FILTER_LLM_API_KEY` | *(Optional)* API key for the filter LLM |
| `FILTER_LLM_MODEL` | *(Optional)* Model name for the filter (default: `haiku`) |

### 3. Expose to the internet

MentraOS needs to reach your bridge. Options:

- **ngrok:** `ngrok http --url=your-static-url 3000`
- **Reverse proxy:** nginx/caddy pointing to port 3000
- **Public VPS:** Just open port 3000

Set the webhook URL in your MentraOS Developer Console to point to your bridge.

### 4. Run

```bash
# Development (hot reload)
bun run dev

# Production
bun run start
```

### 5. Connect your glasses

Open the MentraOS app on your phone ‚Üí start your app ‚Üí the glasses should show "Hex connected." (or your agent's name). Look up for 5 seconds to start listening.

## Usage

### Voice commands

| Command | What it does |
|---------|-------------|
| *Any question or statement* | Sends to your AI, shows reply on display |
| "Copilot mode" / "Copilot on/off" / "Copilot an/aus" | Toggle copilot mode |
| "New session" / "Neue Session" | Reset the AI conversation |

### Head gestures

- **Look up for 5+ seconds** ‚Üí Toggle microphone on/off
- Quick glances up (checking time, dashboard) don't trigger it

### Copilot Mode

In copilot mode, the AI listens to your conversations but stays silent. It only shows a brief hint on the display when it has something genuinely useful to add ‚Äî a fact check, a name, a relevant detail. Perfect for meetings or conversations where you want background context without interruption.

#### LLM Pre-Filter

When a copilot filter LLM is configured (`FILTER_LLM_URL`), every transcript batch goes through a cheap, fast model first (e.g. Claude Haiku, ~200ms per call). The filter decides:

- **SKIP** ‚Äî Casual chitchat, filler words, fragments, garbled transcription ‚Üí logged but never sent to your main AI
- **RELEVANT** ‚Äî Factual claims, questions, the AI being addressed by name, numbers/dates that could be checked ‚Üí forwarded to your main AI with a sliding window of recent conversation context

This typically filters out **85-95% of transcripts**, massively reducing costs while keeping your main AI free for direct interactions.

If the filter LLM is not configured, all copilot transcripts pass through to your main AI (backwards compatible).

#### Conversation Context Window

When a transcript is classified as RELEVANT, the bridge doesn't just send the current snippet ‚Äî it includes the last 5 transcript batches as conversation context (including ones that were filtered as SKIP). This gives your main AI enough context to understand what's being discussed.

#### Transcript Logging

All transcripts (both normal mode and copilot mode) are logged to `transcripts/YYYY-MM-DD.md` in a plain-text, LLM-friendly format:

```
[19:38:05] (copilot) [SKIP] Hm.
[19:38:12] (copilot) [RELEVANT] Der Eiffelturm ist 200 Meter hoch
[19:39:00] (normal) Hex, wie wird das Wetter morgen?
```

Your AI agent can read these files on demand for conversation summaries ("what did we talk about earlier?").

Transcripts are **debounced** (batched over 3-second windows) to avoid flooding the AI with every sentence fragment. If the AI is busy researching something, new transcripts queue up and get processed after the current request finishes ‚Äî nothing gets lost or cancelled.

### Push API

Send messages or images to the glasses programmatically ‚Äî great for reminders, alerts, or integrating with other tools.

**Text push:**
```bash
curl -X POST http://localhost:3001/push \
  -H 'Content-Type: application/json' \
  -d '{"text": "Meeting in 5 minutes", "duration": 10000}'
```

**Bitmap push:**
```bash
curl -X POST http://localhost:3001/push-bitmap \
  -H 'Content-Type: application/json' \
  -d '{"bitmap": "<base64 BMP>", "duration": 10000}'
```

**Mic toggle (for Tasker/automation):**
```bash
curl -X POST http://localhost:3001/mic
```

**Copilot toggle (for Tasker/WearOS):**
```bash
curl -X POST http://localhost:3001/copilot
# ‚Üí {"ok":true,"sessions":1,"copilot":true}
```

**Copilot status:**
```bash
curl http://localhost:3001/copilot
# ‚Üí {"ok":true,"sessions":1,"copilot":false}
```

**Status check:**
```bash
curl http://localhost:3001/status
# ‚Üí {"ok":true,"openclaw":true,"sessions":1,"listening":false,"copilot":false}
```

**Debug / pipeline diagnostics:**
```bash
curl http://localhost:3001/debug
# ‚Üí {
#   "ok": true,
#   "openclaw": true,
#   "totalSessions": 1,
#   "sessions": {
#     "session-id": {
#       "listening": true,
#       "copilot": true,
#       "lastTranscriptAgo": "12s",
#       "copilotPipeline": {
#         "size": 0,
#         "bufferSize": 0,
#         "inflight": false,
#         "totalFiltered": 42,
#         "totalPassed": 5
#       },
#       "progress": 0
#     }
#   }
# }
```

Pipeline fields:
- `size` ‚Äî Transcripts currently in the pipeline (buffer + being filtered + being processed by main AI)
- `bufferSize` ‚Äî Transcripts waiting for the debounce timer
- `inflight` ‚Äî Whether a batch is currently being processed (filter or main AI)
- `totalFiltered` ‚Äî Lifetime count of transcripts filtered out (SKIP) ‚Äî Opus calls saved!
- `totalPassed` ‚Äî Lifetime count of transcripts forwarded to main AI (RELEVANT)
- `progress` ‚Äî Pipeline fill percentage (each item = 20%, caps at 100)

If `PUSH_TOKEN` is set, add `Authorization: Bearer <token>` header or `?token=<token>` query param.

### Bitmap text helper

Generate and push text as a monochrome bitmap (pixel font):

```bash
bun scripts/push-bitmap.js "Hello World" 10000
```

## Display limitations

The G1 has a monochrome (green-on-black) display, roughly 640√ó200 pixels with ~576px usable width. Keep in mind:

- **No color, no grayscale** ‚Äî 1-bit only
- **~5 lines of text** visible at once (~180 characters max per page)
- **No scrolling** ‚Äî the bridge auto-paginates long replies (180 chars/page, 8s per page)
- **No images** in replies ‚Äî text only (but you can push bitmaps via the API)
- AI replies should be **2-3 short sentences max**, no markdown, no emojis

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  G1 Glasses ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ MentraOS     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  This Bridge (Bun/Node)     ‚îÇ
‚îÇ  (mic/hud)  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Cloud (STT)  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
                                         ‚îÇ  ‚îÇ Transcript Logger   ‚îÇ   ‚îÇ
                                         ‚îÇ  ‚îÇ (transcripts/*.md)  ‚îÇ   ‚îÇ
                                         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
                                         ‚îÇ                             ‚îÇ
                                         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
                                         ‚îÇ  ‚îÇ LLM Pre-Filter      ‚îÇ   ‚îÇ
                                         ‚îÇ  ‚îÇ (Haiku, copilot     ‚îÇ   ‚îÇ
                                         ‚îÇ  ‚îÇ  mode only)         ‚îÇ   ‚îÇ
                                         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
                                         ‚îÇ             ‚îÇ RELEVANT     ‚îÇ
                                         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
                                         ‚îÇ  ‚îÇ Push API (:3001)    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ Tasker/WearOS
                                         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
                                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                      ‚îÇ WebSocket
                                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                         ‚îÇ  OpenClaw Gateway          ‚îÇ
                                         ‚îÇ  (your main AI agent)      ‚îÇ
                                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Running as a systemd service

For always-on operation:

```ini
# ~/.config/systemd/user/g1-bridge.service
[Unit]
Description=G1-OpenClaw Bridge
After=network.target

[Service]
Type=simple
WorkingDirectory=/path/to/HexMentraBridge
ExecStart=/path/to/bun run start
Restart=always
RestartSec=5

[Install]
WantedBy=default.target
```

```bash
systemctl --user daemon-reload
systemctl --user enable --now g1-bridge
loginctl enable-linger $USER  # keep running after logout
```

---

## For OpenClaw Agents

If you're an OpenClaw agent and your human wants to set up G1 glasses integration, here's what you need to know.

### What this does for you

This bridge gives you a voice interface through smart glasses. Your human speaks, you see their words as a message (with a special prefix), and your text reply appears on their glasses display. You also get a push API to proactively send messages to the glasses anytime.

### Installation (do this for your human)

```bash
cd /path/to/workspace
git clone https://github.com/johannboehme/HexMentraBridge.git
cd HexMentraBridge
bun install
cp .env.example .env
```

Then edit `.env`:
- `PACKAGE_NAME` and `MENTRAOS_API_KEY` ‚Äî your human gets these from their MentraOS Developer Console
- `OPENCLAW_WS_URL` ‚Äî point to your Gateway WebSocket (usually `ws://localhost:18789` if on the same machine)
- `OPENCLAW_GW_TOKEN` ‚Äî your Gateway master token (check your OpenClaw config)
- `FILTER_LLM_URL`, `FILTER_LLM_API_KEY`, `FILTER_LLM_MODEL` ‚Äî *(optional)* for copilot pre-filtering

Then run the one-time device pairing setup:

```bash
bun scripts/setup-device-auth.ts
```

This generates an Ed25519 keypair, saves it to `.device-auth.json`, and pairs the bridge with your Gateway so it gets `operator.read` + `operator.write` scopes. Without this step, `chat.send` will fail with `missing scope: operator.write` ‚Äî the Gateway strips all scopes from clients without a verified device identity.

> **Note:** `.device-auth.json` contains your private key. It is already in `.gitignore`. Never commit it.

### How messages arrive

Messages from the glasses arrive in your main session with these markers:

**Normal mode:**
- **Prefix:** `‚ö†Ô∏è G1 BRIDGE DISPLAY: Use only 2-3 short sentences, no markdown, no emojis!`
- **Message ID:** starts with `g1-`

**Copilot mode (only RELEVANT transcripts reach you):**
- **Prefix:** `‚ö†Ô∏è G1 COPILOT MODE: The user is having a conversation nearby...`
- **Includes:** Recent conversation context (last 5 transcript batches) + current relevant text
- Reply `NO_REPLY` to stay silent ‚Äî the bridge filters these out

When you see the normal mode prefix, reply with **plain text only**:
- Max 2-3 short sentences
- No markdown (`**`, `#`, `-`, backticks)
- No emojis
- Keep it under ~180 characters (longer replies get auto-paginated)

Your reply routes back automatically through the Gateway WebSocket ‚Üí Bridge ‚Üí glasses display.

### Reading transcripts

All conversations are logged to `transcripts/YYYY-MM-DD.md`. When your human asks "what did we talk about?", read the transcript file for that day. The format is plain text, one line per transcript ‚Äî easy to summarize.

### Proactive push

You can send messages to the glasses anytime without waiting for the user to speak:

```bash
# Check if glasses are connected
curl -s http://127.0.0.1:3001/status

# Push a message
curl -s -X POST http://127.0.0.1:3001/push \
  -H 'Content-Type: application/json' \
  -d '{"text": "Reminder: meeting in 10 min", "duration": 10000}'

# Toggle mic remotely
curl -s -X POST http://127.0.0.1:3001/mic

# Toggle copilot mode
curl -s -X POST http://127.0.0.1:3001/copilot

# Check pipeline status
curl -s http://127.0.0.1:3001/debug
```

Use push for calendar reminders, urgent notifications, weather alerts, or anything time-sensitive.

### Tips for agents

- The display is tiny and monochrome ‚Äî brevity is everything
- Long replies auto-paginate (180 chars/page, 8s per page) but shorter is always better
- Push API has no auth by default (localhost-only) ‚Äî if `PUSH_TOKEN` is set, include it as a Bearer token
- The bridge auto-reconnects to your Gateway WebSocket if it drops
- Transcription works in multiple languages even though the subscription is `en-US`
- Head-up toggle means the mic isn't always on ‚Äî the user activates it deliberately
- In copilot mode, you get conversation context with each RELEVANT transcript ‚Äî use it!

## Changelog

### v0.9.1
- **Device Auth (required)** ‚Äî Bridge now performs proper Ed25519 device pairing with the OpenClaw Gateway. This grants `operator.read` + `operator.write` scopes so `chat.send` works correctly. Run `bun scripts/setup-device-auth.ts` once after install.
- **`scripts/setup-device-auth.ts`** ‚Äî One-shot pairing script: generates keypair, connects to Gateway, saves device token to `.device-auth.json`.
- **Bug fix:** Previously, the bridge connected successfully but all `chat.send` calls failed with `missing scope: operator.write` because the Gateway strips scopes from clients without a verified device identity (even if the master token matches).

### v0.9.0
- **LLM Pre-Filter for Copilot Mode** ‚Äî Configurable cheap LLM (e.g. Claude Haiku) filters copilot transcripts before they reach the main AI. Typically saves 85-95% of API calls.
- **Transcript Logging** ‚Äî All transcripts (normal + copilot) logged to `transcripts/YYYY-MM-DD.md` in plain-text format for later summarization.
- **Conversation Context Window** ‚Äî When a copilot transcript is classified as RELEVANT, the last 5 transcript batches are included as conversation context.
- **Pipeline Diagnostics** ‚Äî Enhanced `/debug` endpoint with `copilotPipeline` object tracking size, buffer, inflight status, and lifetime filtered/passed counts.

### v0.8.1
- FIFO callback queue for reliable run matching
- Copilot debouncing (3s batches)
- Safety timeout for stuck copilot requests
- HTTP API for copilot toggle
- Debug endpoint for pipeline status

### v0.7.1
- Auto-resubscribe on transcription stream errors
- Exponential backoff for reconnection

### v0.6.5
- Initial release with voice chat, copilot mode, notifications, bitmap push, head-up toggle

## License

MIT
